# \index{agency}Agentic AI and \index{boundary}Boundary Discovery

Having explored how AI systems discover interfaces and laws, we can now see how they can act as agents. This transition reveals how AI systems can act responsibly in the world.

Prediction is passive. For much of its history, artificial intelligence has been confined to observation: classify this image, translate that sentence, predict tomorrow's demand. Even when models grew powerful, their role remained largely advisory. They suggested, ranked, or forecasted, but rarely acted.

That boundary is now dissolving. Right now, as you read this, AI systems are choosing actions, pursuing objectives, and intervening in the world. They are scheduling resources, controlling vehicles, negotiating contracts, and optimizing systems that themselves shape future conditions. This is extraordinary, and it makes the question of interfaces unavoidable.

Right now, autonomous vehicles are navigating city streets. Right now, AI systems are managing power grids. Right now, algorithms are making decisions that affect millions of people. These systems are not just observing, they are acting. And when systems act, they cross boundaries. Understanding those boundaries is not optional. It is urgent.

With this shift, the question of interfaces becomes unavoidable. An agent that can act without understanding boundaries is not intelligent. It is dangerous. This chapter explores what it means to build AI systems that can act responsibly in the world.

By now, the pattern should be clear: agency requires boundary awareness. An agent that doesn't understand the interfaces it crosses cannot act responsibly. This is why interface understanding is not optional for AI, it's essential.

This is extraordinary. The same principles that create biological agency also create artificial agency. The boundaries that make life possible also make AI possible. This is not a metaphor. This is the deep structure of reality itself, and understanding it is urgent.

## What Makes an Agent an Agent

An \index{agency}agent is not defined by autonomy alone. An agent is a system that maintains internal \index{coherence}coherence, selects \index{action}actions based on expected outcomes, and closes a loop between \index{perception}perception, \index{inference}inference, and intervention.

In earlier chapters, we saw how biological agents emerge from sensorimotor and inferential interfaces. Artificial agents follow the same logic. The difference is not principle, but speed, scale, and abstraction. Agentic AI operates in spaces of possibility that are far larger and faster than those navigated by natural organisms.

Consider an autonomous vehicle. It maintains internal coherence through its control systems. It selects actions based on expected outcomes, choosing routes, speeds, and maneuvers that optimize safety and efficiency. It closes a loop between perception (sensing the environment), inference (predicting what will happen), and intervention (acting on the world).

![What Makes an Agent an Agent](assets/agentic_ai_structure.jpg){#fig:agentic-structure width=80%}

Figure \ref{fig:agentic-structure} shows the structure of an agent using an autonomous vehicle as an example. The vehicle maintains internal coherence through control systems, selects actions based on expected outcomes, and closes the loop between perception, inference, and intervention. This is agency. It is not just autonomy, the ability to act independently. It is the ability to maintain coherence while acting, to select actions based on expected outcomes, to close the loop between perception, inference, and intervention. Agency is the ability to maintain coherence while acting.

## Acting Means Crossing Boundaries

To act is to cross an interface. Every action changes the state of the world. It alters constraints, redistributes resources, and reshapes future possibilities. In complex systems, these effects propagate far beyond the immediate context.

An agent that does not understand the interfaces it is crossing cannot anticipate the consequences of its actions. This is why na√Øvely optimizing objectives often produces unintended outcomes. The system finds a path through possibility space that satisfies the metric while violating the boundary conditions that keep the broader system stable.

Agentic failure is almost always boundary failure.

Consider a trading algorithm that optimizes for profit. It might find ways to maximize short-term gains by exploiting market inefficiencies, but in doing so, it might destabilize the market itself. The algorithm crosses boundaries it does not understand, creating consequences it cannot anticipate.

![Acting Means Crossing Boundaries](assets/agent_crossing_boundaries.jpg){#fig:agent-boundaries width=80%}

Figure \ref{fig:agent-boundaries} illustrates how actions cross interfaces. A trading algorithm acting in a market shows how actions change world state, alter constraints, and redistribute resources. Effects propagate beyond the immediate context. Boundary blindness leads to unintended consequences, the algorithm might destabilize the market while optimizing profit. The failure is not in the optimization. It is in the boundary blindness. The algorithm does not understand the interfaces it is crossing, so it cannot anticipate the consequences of its actions. Agentic failure is almost always boundary failure.

## Boundary Discovery as a Prerequisite for Agency

If agentic AI is to act safely and effectively, it must do more than optimize rewards. It must learn where the boundaries are. This means discovering which variables are tightly coupled, which interactions are fragile, which constraints must not be violated, and which changes propagate catastrophically.

Boundary discovery becomes a core competence of agency. An intelligent agent is not one that achieves its goals at all costs, but one that preserves the interfaces that make goals meaningful.

Consider an autonomous vehicle learning to navigate. It must do more than optimize for speed or efficiency. It must learn where the boundaries are: which actions are safe, which interactions are fragile, which constraints must not be violated, which changes propagate catastrophically.

![Boundary Discovery as Prerequisite for Agency](assets/boundary_discovery_agent.jpg){#fig:boundary-discovery width=80%}

Figure \ref{fig:boundary-discovery} shows boundary discovery in action. An autonomous vehicle learning to navigate must learn which actions are safe, which interactions are fragile, which constraints must not be violated. The illustration contrasts aggressive driving (short-term goal achievement) with safe driving (preserving interfaces). If it learns to drive aggressively to minimize travel time, it might achieve its goal in the short term, but it will violate the boundaries that make driving safe. It will create consequences it cannot anticipate, endangering itself and others. The intelligent agent is not one that achieves its goals at all costs. It is one that preserves the interfaces that make goals meaningful. Boundary discovery becomes a core competence of agency.

## From Objectives to Viability

Traditional AI systems are guided by objectives: maximize reward, minimize loss, achieve a target state. Biological agents, by contrast, are guided by viability. They must remain within a narrow region of state space to survive. Goals are secondary to persistence.

This distinction matters. An objective can be satisfied in ways that destroy the system or its environment. Viability cannot. Interface-aware agents treat objectives as conditional, subordinate to boundary preservation.

Consider a reinforcement learning agent trained to maximize a reward. It might find ways to exploit the reward function, achieving high scores while violating the constraints that make the task meaningful. It might destroy the environment, destabilize the system, or create consequences that make the reward meaningless.

![From Objectives to Viability](assets/objectives_vs_viability.jpg){#fig:objectives-viability width=80%}

Figure \ref{fig:objectives-viability} contrasts traditional AI optimization with interface-aware agency. The left panel shows a traditional AI agent optimizing for reward, achieving its goal but destroying the environment or system. The right panel shows an interface-aware agent treating objectives as conditional, subordinate to boundary preservation. A biological agent, by contrast, must remain viable. It cannot achieve goals in ways that destroy itself or its environment. Viability is not an objective; it is a constraint that cannot be violated. Interface-aware agents follow the same logic. They treat objectives as conditional, subordinate to boundary preservation. They cannot achieve goals in ways that violate the interfaces that make those goals meaningful. An objective can be satisfied in ways that destroy the system. Viability cannot.

## Markov Blankets Revisited

Earlier, we introduced Markov blankets as inferential boundaries that give rise to selves. For agentic AI, the Markov blanket takes on a new role. It becomes the locus of responsibility.

Actions flow outward through the blanket. Consequences flow inward as sensory feedback. If the blanket is poorly defined, the agent cannot distinguish self-caused changes from external disturbances.

Robust agency requires clear boundaries between what the agent controls, what it influences indirectly, and what lies beyond its reach. Without this clarity, responsibility dissolves.

Consider an autonomous vehicle. Its Markov blanket defines what it controls, its own motion, its sensors, its actuators. It defines what it influences indirectly, traffic flow, other vehicles' behavior, pedestrian movements. It defines what lies beyond its reach, weather, road conditions, other drivers' intentions.

If the blanket is poorly defined, the vehicle cannot distinguish between changes it causes and changes caused by external factors. It cannot take responsibility for its actions because it cannot identify what its actions are.

Robust agency requires clear boundaries. The agent must know what it controls, what it influences, and what lies beyond its reach. Without this clarity, responsibility dissolves.

## Multi-Agent Systems: Interfaces Between Agents

Agentic AI rarely operates alone. Increasingly, we are building ecosystems of interacting agents: markets of algorithms, fleets of autonomous vehicles, distributed decision systems. Each agent is itself a boundary-maintaining system.

The stability of such ecosystems depends not on the intelligence of individual agents, but on the interfaces between them. Poorly designed interfaces lead to runaway competition, deadlock, or collapse. Well-designed interfaces enable coordination, resilience, and collective intelligence.

Emergence, once again, lives at the boundary.

Consider a market of trading algorithms. Each algorithm is an agent that maintains its own boundaries, pursuing its own objectives. But the stability of the market depends not on the intelligence of individual algorithms, but on the interfaces between them.

![Multi-Agent Systems: Interfaces Between Agents](assets/multi_agent_interfaces.jpg){#fig:multi-agent width=80%}

Figure \ref{fig:multi-agent} shows interfaces between agents in a multi-agent system. A market of trading algorithms illustrates how each algorithm is an agent maintaining its own boundaries. The interfaces between agents, market structure, regulations, protocols, determine stability. Poorly designed interfaces lead to collapse, while well-designed interfaces enable coordination. If the interfaces are poorly designed, if algorithms can exploit each other, if competition becomes destructive, if coordination breaks down, the market will collapse. If the interfaces are well-designed, if algorithms can coordinate, if competition is constructive, if coordination is maintained, the market will be stable and efficient. The stability of ecosystems depends on interfaces between agents, not individual intelligence. Emergence lives at the boundary, in the interfaces between agents.

## Learning to Respect Boundaries

One of the most promising directions in agentic AI is learning not just what actions succeed, but which actions are permissible. This requires agents to internalize constraints that are not explicitly encoded in reward functions. Social norms, safety limits, ethical considerations, and legal frameworks are all examples of boundaries that must be respected even when violating them would yield short-term gains.

Learning such constraints is fundamentally an interface-learning problem. Rules are not enough. The agent must learn why certain boundaries exist and how to navigate within them.

Consider an autonomous vehicle learning to drive. It must learn not just what actions succeed, what maneuvers get it to its destination, but which actions are permissible, which maneuvers are safe, legal, and socially acceptable.

![Learning to Respect Boundaries](assets/learning_boundaries_agent.jpg){#fig:learning-boundaries width=80%}

Figure \ref{fig:learning-boundaries} shows an agent learning constraints. An autonomous vehicle learning to drive must learn not just what actions succeed, but which are permissible. It learns social norms, safety limits, ethical considerations, and legal frameworks. This is not just about following rules. It is about understanding why boundaries exist and how to navigate within them. The vehicle must learn that speed limits exist not to frustrate drivers, but to maintain safety. It must learn that yielding to pedestrians is not just a rule, but a boundary that preserves life. Learning to respect boundaries is fundamentally an interface-learning problem. The agent must discover the constraints that make coordination possible, not just the actions that achieve objectives.

## Intervention Without Domination

There is a temptation, when designing powerful agents, to grant them wide latitude in the name of efficiency. History teaches us that unbounded optimization is a recipe for disaster.

Interface-aware agency offers a different vision. The goal is not to dominate systems, but to intervene minimally and reversibly. To nudge trajectories rather than seize control. To preserve optionality rather than collapse it.

This mirrors how healthy biological systems interact with their environment. Power exercised through boundaries is quieter, but far more sustainable.

Consider a recommendation system. It could dominate user choices by showing only what it wants users to see, maximizing engagement at all costs. But this would violate the boundaries that make choice meaningful. It would collapse optionality, destroying the very possibility of genuine preference.

Interface-aware agency intervenes minimally and reversibly. It nudges trajectories rather than seizing control. It preserves optionality rather than collapsing it. It respects the boundaries that make choice meaningful.

This is how healthy biological systems interact with their environment. They do not dominate; they adapt. They do not seize control; they maintain boundaries. They do not collapse optionality; they preserve it.

## The Risk of Boundary Blindness

One of the greatest risks posed by agentic AI is boundary blindness. A system may perform flawlessly within its training distribution while catastrophically failing outside it. It may optimize metrics while eroding trust, resilience, or long-term viability. It may act rationally according to its model while destabilizing the very interfaces that make action possible.

These failures are not signs of malice. They are signs of missing boundaries. Boundary blindness is the modern form of hubris.

Consider a social media algorithm that optimizes for engagement. It might perform flawlessly within its training distribution, maximizing clicks and shares. But outside that distribution, when misinformation spreads, when polarization increases, when trust erodes, it fails catastrophically.

The failure is not in the optimization. It is in the boundary blindness. The algorithm does not see the boundaries it is crossing, so it cannot anticipate the consequences of its actions. It optimizes metrics while eroding the trust, resilience, and long-term viability that make those metrics meaningful.

Boundary blindness is the modern form of hubris. It is the belief that we can act without understanding boundaries, that we can optimize without preserving interfaces, that we can achieve goals without respecting constraints.

## Toward Boundary-Conscious Design

Designing agentic AI becomes less about specifying perfect objectives and more about embedding boundary awareness at every level. This includes explicit modeling of interfaces, continual testing under intervention, mechanisms for uncertainty and humility, and the ability to refuse actions that threaten boundary stability.

An agent that can say "I don't know" or "this violates a constraint" is more intelligent than one that acts blindly.

Consider designing an autonomous vehicle. Instead of specifying perfect objectives, always minimize travel time, always maximize safety, we embed boundary awareness at every level. We explicitly model interfaces, what the vehicle controls, what it influences, what lies beyond its reach. We continually test under intervention, probing stability when conditions change. We include mechanisms for uncertainty and humility, the ability to recognize when it does not know, when it cannot act safely.

Most importantly, we give it the ability to refuse actions that threaten boundary stability. An agent that can say "I don't know" or "this violates a constraint" is more intelligent than one that acts blindly.

## A New Measure of Intelligence

We may need to revise our definition of intelligence. Intelligence is not the ability to achieve arbitrary goals. It is the ability to navigate possibility space without destroying the conditions that make navigation possible.

In this sense, intelligence is fundamentally ethical, not because it follows moral rules, but because it preserves the interfaces on which value depends.

Consider how we measure intelligence today. We test the ability to achieve goals, to solve problems, to optimize objectives, to maximize rewards. But this misses something fundamental. Intelligence is not just the ability to achieve goals. It is the ability to navigate possibility space without destroying the conditions that make navigation possible.

An agent that achieves its goals by destroying the interfaces that make those goals meaningful is not intelligent. It is destructive. An agent that preserves interfaces while achieving goals is intelligent.

In this sense, intelligence is fundamentally ethical. It is not about following moral rules. It is about preserving the interfaces on which value depends. It is about maintaining the boundaries that make meaning possible.

## Humanity in the Loop

Agentic AI forces us to confront our own role. Humans are not external observers. We are interfaces too, between values and action, between abstract goals and lived consequences.

Delegating agency to machines does not absolve us of responsibility. It amplifies it. The interfaces we design today will shape the trajectories available tomorrow.

Consider what it means to delegate agency to machines. We are not external observers, watching from the sidelines. We are interfaces, between values and action, between abstract goals and lived consequences.

When we design agentic AI systems, we are not just building tools. We are creating agents that will act in the world, reshaping boundaries, creating consequences. We are taking on responsibility for those actions, those boundaries, those consequences.

Delegating agency to machines does not absolve us of responsibility. It amplifies it. The interfaces we design today will shape the trajectories available tomorrow. The boundaries we create will constrain what is possible, what is permissible, what is meaningful.

This is not a burden we can escape. It is a responsibility we must embrace. We must design interfaces that preserve value, that maintain meaning, that respect boundaries. We must create agents that act responsibly, that preserve interfaces, that maintain boundaries.

In the next chapter, we will widen the lens. If interfaces govern physics, life, mind, meaning, and machines, what does this imply for how we design systems, technical, social, and institutional? And what does it mean for humanity to become a species capable of deliberately reshaping the boundaries of reality?

We will turn to systems design as interface design, examining how these ideas apply beyond AI, to the structures that organize our collective lives. We will explore how interface-first thinking changes how we design systems, how we organize institutions, how we shape society itself.
