# Discovering \index{law}Laws, Not Just Data

Having seen how AI systems discover interfaces, we can now explore how they discover laws. This transformation shows how AI can become a tool for scientific discovery.

For centuries, science has pursued a singular ambition: to discover the laws that govern reality. From Newton's universal gravitation to Einstein's relativity, from Maxwell's equations to Schr√∂dinger's wave function, scientists have sought the fundamental constraints that shape how the universe behaves.

Right now, as you read this, AI systems are rediscovering laws that took human scientists centuries to find. In 2019, systems at MIT learned Hamiltonian mechanics from trajectory data alone. In 2021, AI systems discovered symmetries in particle physics data that had taken physicists decades to identify. This is happening in real-time, in silicon instead of flesh, in weeks instead of centuries. This is extraordinary, and it demands a new kind of awareness.

These laws are not merely patterns. They are constraints that hold across time, scale, and circumstance. They tell us not just what happens, but what cannot happen. They define the boundaries within which the universe unfolds. When a ball falls, it cannot fall faster than gravity allows. When energy transforms, it cannot be created or destroyed. When particles interact, they cannot violate quantum constraints. These are not observations; they are boundaries that reality itself enforces.

This might seem abstract, but here's why it matters: if AI systems can discover laws, they can become partners in scientific discovery. They can explore possibility spaces that humans cannot navigate alone. They can find interfaces that we have not yet seen. This is not just about better AI, it's about a new form of scientific collaboration.

To answer this, we must first be clear about what a law really is.

## What Makes a Law a Law

A \index{law}law is not a formula written on paper. It is not even an equation, elegant as equations may be. A law is a \index{stability}stable \index{constraint}constraint that survives intervention, variation, and abstraction.

![What Makes a Law a Law: Stability Across Contexts](assets/law_remains.jpg){#fig:law-remains width=80%}

Figure \ref{fig:law-remains} illustrates how laws remain stable across variable contexts and interventions. Despite noise, variation, and abstraction, the law persists as a stable constraint, an invariant interface. A true law remains intact when the system is observed differently, when the scale changes, when noise is introduced, when implementation details vary. Newton's laws survive whether we track planets or pendulums. Conservation laws hold regardless of the materials involved. Thermodynamic principles apply to engines, ecosystems, and economies alike.

What these laws share is not mathematical form, but interface stability. They describe boundaries that remain invariant across contexts. Consider conservation of energy. It holds across scales, from quantum mechanics to cosmology, from mechanical systems to biological ones. A photon emitted by a distant star, a chemical bond breaking in a cell, a car accelerating on a highway, all must respect the same constraint. We can add or remove energy, but the law still holds. We can represent energy as kinetic, potential, thermal, chemical, or nuclear, but the constraint remains. This stability is what makes it a law, an interface that remains invariant across contexts. The law is not in the specific form of energy, but in the constraint that total energy remains constant.

## Why Correlation Is Not Enough

If laws are stable constraints, then correlation alone cannot yield them. Modern machine learning excels at correlation. Given enough data, it can predict outcomes with astonishing accuracy. But correlation alone does not yield laws.

![Why Correlation Is Not Enough](assets/correlation_vs_law.jpg){#fig:correlation-vs-law width=80%}

Figure \ref{fig:correlation-vs-law} illustrates the crucial distinction between correlation and law. Correlations are brittle and break when conditions change, while laws remain stable constraints that survive intervention. Correlations are brittle. They break when conditions change. They fail under intervention. They do not tell us what must remain true. This is why purely data-driven models struggle in scientific discovery. They can interpolate but not extrapolate. They can predict but not explain. Explanation requires understanding constraints.

Consider a model that predicts stock prices based on historical data. It might achieve 95% accuracy on the training data, recognizing patterns like "tech stocks rise on Fridays" or "energy stocks correlate with oil prices." But when market conditions change, a financial crisis, new regulations, a pandemic, the model fails catastrophically. The correlations it learned were real, but they were not laws. They were patterns that held under specific conditions and broke when those conditions changed.

A law, by contrast, would tell us what constraints must hold regardless of market conditions. It would tell us what cannot happen, perhaps that total market value cannot increase without corresponding value creation, or that arbitrage opportunities cannot persist indefinitely. These constraints would survive intervention, new regulations, market manipulation, or economic shocks. They would survive abstraction, whether we measure value in dollars, euros, or abstract units. The law is not in the specific patterns, but in the constraints that limit what patterns are possible.

## Laws as Interfaces in Possibility Space

If laws are stable constraints, where do they exist? From the perspective developed in this book, a law is an interface in the space of possibilities. It defines a boundary that separates viable behaviors from non-viable ones. It does not specify exact trajectories; it constrains the set of allowed trajectories.

![Laws as Interfaces in Possibility Space](assets/laws_as_interfaces_possibility_space.jpg){#fig:laws-possibility-space width=80%}

Figure \ref{fig:laws-possibility-space} illustrates how laws function as boundaries in possibility space, separating viable behaviors from non-viable ones without specifying exact trajectories. This is why laws feel abstract and universal. They are not descriptions of particular events. They are descriptions of what persists under change. Consider the second law of thermodynamics. It does not tell us exactly how entropy will increase in a particular system, whether a cup of coffee will cool in five minutes or ten, whether a chemical reaction will proceed quickly or slowly. Instead, it tells us that entropy cannot decrease in an isolated system. It defines a boundary in possibility space, a constraint that separates what can happen from what cannot. 

This boundary is an interface, not a pattern in the data, but a constraint on what data can occur. You can observe a million different systems, each with different entropy increases, but none will violate this constraint. The law doesn't predict specific outcomes; it constrains the space of possible outcomes. Discovering a law is discovering an interface that remains stable across interventions, an invariant boundary that reality itself respects.

## Intervention as the Test of Reality

How do we know if a discovered constraint is truly a law? One of the defining features of scientific laws is that they survive intervention. You can push a system, perturb it, reconfigure it, and the law still holds. This robustness is what distinguishes law from coincidence.

![Intervention as the Test of Reality](assets/intervention_tests_laws.jpg){#fig:intervention-tests width=80%}

Figure \ref{fig:intervention-tests} illustrates how intervention tests boundaries, revealing which constraints are stable and which are fragile. Intervention tests boundaries. It reveals which constraints are stable and which are fragile. An interface that collapses under small perturbations is not a law. An interface that remains predictive when variables are changed, when representations are altered, when contexts shift, that interface begins to look like a law.

Consider testing whether a discovered constraint is a law. We might perturb the system, heat it, cool it, shake it, compress it. We might change the conditions, vary pressure, alter composition, modify geometry. We might alter the variables, swap materials, change scales, introduce noise. And we see if the constraint still holds. If it does, we have evidence that it is a law. If it does not, we have evidence that it is merely a correlation.

This is how science has always worked. Galileo didn't discover the law of falling bodies by simply watching objects fall. He rolled balls down inclined planes, varied the angles, changed the materials, tested different weights. Newton didn't discover universal gravitation by passively observing the moon. He calculated, tested, and verified that the same force that makes an apple fall also keeps the moon in orbit. Laws are not discovered by passive observation alone. They are discovered by active intervention, by testing whether constraints survive when we push against them. The more we push, the more we test, the more confident we become that we've found a true law, an interface that reality itself enforces.

## The Role of Abstraction

If laws are interfaces, why do they appear in different forms? Laws are abstract not because they are removed from reality, but because they ignore irrelevant details. Abstraction is a form of boundary-making. It separates what matters from what does not.

![The Role of Abstraction](assets/abstraction_same_interface.jpg){#fig:abstraction width=80%}

Figure \ref{fig:abstraction} illustrates how the same law appears in different representations, with all forms connecting to the same underlying interface. When a system abstracts correctly, it preserves the interface while discarding internal complexity. This explains why different scientific fields can discover the "same" law in different forms. Consider conservation of momentum. In classical mechanics, it is expressed as the constancy of mass times velocity, when two billiard balls collide, their total momentum before equals their total momentum after. In quantum mechanics, it is expressed as the constancy of wave function properties, the momentum operator commutes with the Hamiltonian, preserving momentum in quantum systems. In relativity, it is expressed as the constancy of four-momentum, a four-vector that combines energy and momentum, remaining constant in spacetime. 

These are different representations, but they describe the same interface, the same constraint that remains invariant across contexts. A physicist working with billiard balls, a quantum physicist studying electron behavior, and a relativist calculating particle collisions, all are working with the same underlying constraint, just expressed differently. The interface is the same; the representation differs. The abstraction preserves what matters, the conservation of momentum, while discarding what does not, the specific mathematical form, the scale, the context. This is why the same law can be discovered independently in different fields, using different tools and languages.

## AI as an Interface Explorer

If laws are interfaces in possibility space, can AI discover them? Artificial intelligence, when properly guided, can act as an explorer of possibility space. Instead of optimizing for prediction accuracy alone, an AI system can be tasked with identifying minimal variable sets that preserve predictive power, testing whether adding variables materially improves performance, shrinking models until performance degrades, and probing stability under intervention.

![AI as an Interface Explorer](assets/ai_interface_explorer.jpg){#fig:ai-explorer width=80%}

Figure \ref{fig:ai-explorer} illustrates how AI systems explore possibility space, pruning variables and testing stability to identify candidate interfaces. Consider an AI system designed to discover physical laws. It might start with a large set of variables, position, velocity, acceleration, mass, force, energy, temperature, pressure, and dozens of others. It gradually prunes them, testing whether removing a variable breaks the interface. Does the constraint still hold if we ignore temperature? If we ignore pressure? The system systematically eliminates variables that don't affect the core constraint.

It tests whether adding more data improves performance or just adds noise. Does observing a million more collisions reveal new constraints, or just confirm existing ones? It probes stability by perturbing the system, varying masses, changing velocities, altering conditions, and seeing if the discovered constraints still hold. A constraint that breaks when mass doubles is not a law. A constraint that holds whether we're dealing with electrons or planets, whether we're at absolute zero or stellar temperatures, that begins to look like a law.

What remains after this process are candidates for interfaces. The most stable of these candidates are law-like, interfaces that remain stable across interventions and abstractions. They survive when we change variables, when we add noise, when we alter conditions. They are not patterns in the data; they are constraints on what data can occur.

This is extraordinary. AI systems are not just finding patterns; they are discovering the same interfaces that reality itself enforces. They are not copying human science; they are exploring the same landscape of possibilities and converging on the same boundaries. This convergence tells us something profound: the world is structured, and these structures are discoverable. The interfaces are not hidden; they are waiting to be found.

## Rediscovering Known Laws

In recent years, AI systems have rediscovered classical laws of physics from raw data: conservation of momentum, Hamiltonians, symmetries. In 2019, researchers at ETH Zurich trained a neural network on videos of moving objects and watched it rediscover conservation of momentum. In 2020, systems at MIT learned Hamiltonian mechanics from trajectory data alone. In 2021, AI systems discovered symmetries in particle physics data that had taken physicists decades to identify.

These successes are often presented as novelties or curiosities, impressive demonstrations of machine learning, but not fundamentally new. But they are more than that. They demonstrate that laws leave discoverable signatures in data when viewed through the right lens. Those signatures are not patterns of outcomes, not "objects usually move in straight lines" or "energy is often conserved." They are patterns of constraint, "momentum must be conserved" or "energy cannot be created." The AI doesn't learn what usually happens; it learns what cannot happen. It discovers the boundaries that limit possibility, not the patterns that describe typical behavior.

![Rediscovering Known Laws](assets/ai_rediscovering_laws.jpg){#fig:ai-rediscovering width=80%}

Figure \ref{fig:ai-rediscovering} illustrates how AI systems rediscover classical laws from data, finding patterns of constraint rather than patterns of outcomes. AI systems rediscover laws because laws are interfaces that cannot be bypassed. Any system that successfully navigates a domain must, in some form, respect those boundaries. Consider an AI system that learns to predict the motion of objects. It might observe thousands of collisions, billiard balls, particles in a collider, celestial bodies. If it discovers conservation of momentum, it is not just finding a pattern like "collisions usually conserve momentum." It is discovering an interface, a constraint that governs how objects can move. This interface is not arbitrary. It is a stable boundary in possibility space, a constraint that reality itself enforces.

The system rediscovers the law not because it is programmed to, but because the law is an interface that cannot be bypassed. Any system that successfully predicts motion must respect this boundary. If the AI tried to predict that two colliding objects could gain momentum from nowhere, its predictions would fail. The law constrains what predictions are possible. The AI doesn't choose to respect the law; it must respect the law to make accurate predictions. The interface is not optional; it is necessary.

## Discovering New Laws

The more provocative possibility is that AI may discover laws we do not yet know. Not because machines are more intelligent, but because they can explore possibility space differently. They can test vast numbers of hypothetical boundaries, interventions, and abstractions that would be impractical for humans.

![Discovering New Laws](assets/ai_discovering_new_laws.jpg){#fig:discovering-new-laws width=80%}

Figure \ref{fig:discovering-new-laws} illustrates how AI systems discover previously unknown laws by systematically exploring possibility space and uncovering constraints that were always there. If a constraint consistently emerges as necessary for stability across many contexts, it may signal a new law, or at least a new effective law. Science becomes less about guessing equations and more about boundary discovery. Consider a domain where we suspect there are laws but have not yet discovered them, perhaps the behavior of complex ecosystems, the dynamics of neural networks, or the patterns of social coordination.

An AI system could explore this domain systematically, testing many possible constraints, thousands, millions, even billions of hypothetical boundaries. It could probe stability under intervention, varying conditions, introducing perturbations, changing scales. It could identify interfaces that remain invariant, constraints that hold whether we're studying a small ecosystem or a large one, whether we're observing for days or years, whether we're in a lab or the wild. If it discovers a constraint that consistently holds across contexts, scales, and interventions, we might have found a new law. The AI would not have invented it; it would have uncovered what was already there, waiting to be discovered. The constraint was always operating, shaping the domain's behavior; we just hadn't recognized it yet.

## Effective Laws and Layered Reality

Not all laws are fundamental. Many laws are effective: they hold within certain scales, conditions, or domains. Thermodynamics does not replace mechanics. Biology does not replace chemistry. Economics does not replace psychology. Each domain has its own interfaces.

![Effective Laws and Layered Reality](assets/layered_laws_nano_to_macro.jpg){#fig:layered-laws width=80%}

Figure \ref{fig:layered-laws} illustrates how laws change across scales, from quantum/nano scale to macro scale, showing how interfaces stack to create a hierarchy of lawful interfaces. Consider how laws change across scales. At the quantum scale (10^-10 meters), quantum mechanics governs, particles exist in superpositions, measurements collapse wave functions, entanglement creates non-local correlations. At the atomic scale (10^-9 meters), chemistry emerges, atoms bond according to electron configurations, molecules form stable structures, reactions follow thermodynamic constraints. At the molecular scale (10^-6 meters), biology emerges, proteins fold into functional shapes, cells maintain homeostasis, organisms reproduce and evolve. At the organism scale (10^-1 meters), psychology emerges, brains process information, behavior adapts to environments, learning creates new responses. At the social scale (10^3 meters and beyond), economics emerges, markets coordinate exchange, institutions stabilize behavior, cultures evolve over generations.

Each scale has its own interfaces, its own constraints that govern behavior. These interfaces are not arbitrary. They are stable boundaries that emerge from the interactions at lower scales. The laws of chemistry don't replace quantum mechanics; they emerge from it. The laws of biology don't replace chemistry; they build upon it. Each scale adds new constraints while preserving the old ones, creating a hierarchy of lawful interfaces.

Reality becomes a hierarchy of lawful interfaces rather than a single unified equation. AI-assisted discovery can help map these layered laws, revealing how constraints change across scales and how interfaces stack. It could help us understand not just what the laws are, but how they relate to each other.

## The End of Purely Human Science?

This raises an unsettling question. If machines can discover laws, what becomes of human scientists? The answer is not replacement, but partnership. Machines excel at exploring vast spaces and testing hypotheses at scale. Humans excel at interpretation, judgment, and conceptual synthesis.

![The End of Purely Human Science?](assets/human_ai_partnership_science.jpg){#fig:human-ai-partnership width=80%}

Figure \ref{fig:human-ai-partnership} illustrates the collaborative partnership between humans and AI in law discovery. Law discovery becomes a collaborative process: machines propose candidate interfaces; humans evaluate their meaning, scope, and implications. Consider how this partnership might work. An AI system explores a domain, testing many possible constraints, perhaps millions of hypothetical boundaries in a complex biological system. It proposes candidates for laws, constraints that consistently hold across contexts. Human scientists then evaluate these candidates, interpreting their meaning, what does this constraint tell us about how the system works? They assess their scope, does this hold only in specific conditions, or is it more general? They explore their implications, what does this mean for our understanding of the domain? What new questions does it raise?

The machines do the exploration, systematically testing possibilities that would take humans lifetimes to examine. The humans do the interpretation, bringing judgment, creativity, and conceptual understanding that machines lack. Together, they discover laws that neither could discover alone. The AI finds constraints that humans might never have thought to test. The humans understand what those constraints mean, how they relate to existing knowledge, and what they imply for future research. Science becomes more reflective, not less, the partnership between human insight and machine exploration creates a new form of scientific discovery.

## Explanation Revisited

One of the deepest anxieties about AI-driven science is the fear of losing explanation. If a machine produces a model we cannot understand, have we truly learned anything? The interface perspective reframes this concern. An explanation is not a full internal model. It is an account of the constraints that matter.

![Explanation Revisited](assets/explanation_as_interface.jpg){#fig:explanation-interface width=80%}

Figure \ref{fig:explanation-interface} illustrates how explanation shifts from understanding internal details to understanding external constraints. Consider an AI system that discovers a law but uses a complex neural network to represent it, perhaps a deep network with hundreds of layers and millions of parameters. We might not understand the internal workings of the network, how each neuron processes information, how the layers transform representations, how the weights encode knowledge. But we can understand the interface, the constraint that governs behavior. 

If the AI discovers that "in this domain, quantity X must always equal quantity Y," we understand the constraint even if we don't understand how the network learned it. This interface is the explanation. It tells us what must remain stable for the law to hold. It tells us what constraints govern the domain. We do not need to understand the internal machinery, the neural pathways, the weight matrices, the activation functions, to understand the law. We need to understand the constraint, the boundary, the interface. Understanding shifts from internal detail to external constraint. We explain not by describing how the system works internally, but by describing what constraints it must respect externally.

## Toward a New Scientific Method

We may be witnessing the early stages of a new scientific method. Instead of proposing theories first and testing them later, we collect interaction data, search for stable interfaces, test them under intervention, and formalize those that persist. Theory becomes the articulation of discovered boundaries.

![Toward a New Scientific Method](assets/new_scientific_method.jpg){#fig:new-method width=80%}

Figure \ref{fig:new-method} illustrates the new scientific method: collect data, search for stable interfaces, test under intervention, and formalize those that persist. Consider how this new method might work. We collect data about interactions in a domain, perhaps millions of observations of how systems behave, how they respond to changes, how they evolve over time. We use AI to search for stable interfaces, constraints that remain invariant across contexts. The AI systematically tests thousands of possible constraints, identifying those that consistently hold.

We test these interfaces under intervention, varying conditions, introducing perturbations, changing scales. Does the constraint still hold when we double the temperature? When we change the materials? When we observe at different scales? We probe their stability, seeing which constraints survive and which break. We formalize those that persist, articulating them as laws, not as equations we've invented, but as boundaries we've discovered.

The creativity is not in inventing equations, but in interpreting interfaces. What do these boundaries mean? What do they tell us about the domain? How do they relate to other laws? A discovered constraint might connect previously separate fields, revealing deep unity where we saw only difference. It might challenge existing theories, forcing us to reconsider what we thought we knew. It might open new questions, pointing toward domains we haven't yet explored. This method does not eliminate creativity. It relocates it, from inventing equations to interpreting interfaces, from mathematical invention to conceptual understanding.

## A Subtle Humility

There is something humbling in this picture. Laws are not truths we impose on reality. They are constraints reality imposes on us. AI does not invent laws. It uncovers what was already there, waiting to be respected.

![A Subtle Humility](assets/laws_reality_imposes.jpg){#fig:humility width=80%}

Figure \ref{fig:humility} illustrates how laws are constraints that reality imposes, not truths we impose on reality. Consider what this means for science. We are not discovering laws by imposing our theories on reality. We are discovering laws by uncovering the constraints that reality imposes on us. The laws were always there, waiting to be discovered. Conservation of energy was operating long before humans existed. The second law of thermodynamics was shaping the universe before life began. Quantum mechanics was governing particle behavior before we had the mathematics to describe it.

AI helps us discover them not by being more intelligent, but by being able to explore possibility space more systematically. It can test millions of hypotheses in the time it takes a human to test one. It can examine data at scales and resolutions that would overwhelm human analysis. It helps us uncover what was already there, waiting to be respected. The constraint was always operating; we just hadn't recognized it yet.

This humility is a strength because it aligns us with reality. We are not trying to impose our theories on the world, forcing reality to fit our equations, our models, our expectations. We are trying to discover the constraints that the world imposes on us, learning what boundaries reality itself enforces, what limits it sets, what possibilities it allows. This alignment makes our science more robust, more reliable, more true to the nature of reality itself.

In the next chapter, we turn from discovery to action. If interfaces govern reality, and if AI can learn them, what happens when systems begin to act on the world with this knowledge? What new responsibilities arise when intelligence can reshape boundaries deliberately?

We will examine agentic AI and boundary discovery, confronting the ethical and practical implications of machines that do more than predict, they intervene. These systems will not just discover interfaces; they will act through them, reshaping the boundaries that govern interaction.

This raises profound questions about responsibility, control, and the future of agency itself. When machines can discover and reshape interfaces, what becomes of human agency? What becomes of our responsibility for the boundaries we create?
